{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow dataset\n",
    "- Simple LSTM Network\n",
    "- Long sequences to short chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.0MB 223kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.16.1)\n",
      "Collecting pytz>=2011k (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 168kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.24.2 pytz-2019.3\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mne\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/af/9c64ac8f75b1c932ca5fb16bc27740cd9b9817f9173a6608ae999e82bb6a/mne-0.20.0-py3-none-any.whl (6.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.6MB 841kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.5/dist-packages (from mne) (1.16.1)\n",
      "Collecting scipy>=0.17.1 (from mne)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/60/8cbf00c0deb50a971e6e3a015fb32513960a92867df979870a454481817c/scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.0MB 592kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: scipy, mne\n",
      "Successfully installed mne-0.20.0 scipy-1.4.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/ec/32310181e803f5d22e0dd33eb18924489b2f8d08cf5b6e116a93a6a5d1c6/scikit_learn-0.22.2.post1-cp35-cp35m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.0MB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.16.1)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 3.3MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-0.14.1 scikit-learn-0.22.2.post1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/86/43b8c9138ef4c2a1c492fee92792c83c13799d0e2061ff810d3826d06cd1/seaborn-0.9.1-py2.py3-none-any.whl (216kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.16.1)\n",
      "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.5/dist-packages (from seaborn) (3.0.2)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas>=0.17.1->seaborn) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas>=0.17.1->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (2.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas>=0.17.1->seaborn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->seaborn) (40.8.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K    100% |████████████████████████████████| 378kB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.1)\n",
      "Collecting pyyaml (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 390kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.3.1 pyyaml-5.3.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 698kB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.45.0\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие каналы содержат "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "- Один сэмпл содержит чанк сигнала, чанки не пересекаются\n",
    "- Считывание происходит из случайных файлов из списка\n",
    "- Чанки рандомизированы:\n",
    "    - Учесть рандомизацию по номеру пациента, сессии, времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"file\", \"start\", \"end\", \"label\", \"confidence\"]\n",
    "train_df = pd.read_csv(\"../_DOCS/ref_train.txt\", sep=\" \", names=header)\n",
    "val_df = pd.read_csv(\"../_DOCS/ref_dev.txt\", sep=\" \", names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(full_file):\n",
    "    parts = full_file.split(\"_\")\n",
    "    patient = int(parts[0])\n",
    "    session = int(parts[1][1:])\n",
    "    file = int(parts[2][1:])\n",
    "    return [patient, session, file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 2, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_file(\"00000258_s002_t000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_file_info(df):\n",
    "    values = np.array(df[\"file\"].apply(preprocess_file).tolist())\n",
    "    files_df = pd.DataFrame(values, columns=[\"patient\", \"session\", \"chunk\"], index=df.index)\n",
    "    return df.merge(files_df, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = append_file_info(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = append_file_info(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add other electrode formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_files(df, dataset):\n",
    "    paths = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(\"../edf/{}\".format(dataset)):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            if \".edf\" in file:\n",
    "                name = file.split(\".\")[0]\n",
    "                paths[name] = os.path.abspath(root) + \"/\" +  file\n",
    "    \n",
    "    df[\"full_path\"] = df[\"file\"].apply(paths.get)\n",
    "    return df[df[\"full_path\"].apply(lambda x: \"01_tcp_ar\" in str(x))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = attach_files(train_df, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = attach_files(val_df, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients = train_df[\"patient\"].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_patients = val_df[\"patient\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df[~val_df[\"patient\"].isin(train_patients)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bckg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bckg_files(df):\n",
    "    files_with_seizures = df[df[\"label\"] == \"seiz\"][\"file\"].unique()\n",
    "    return df[df[\"file\"].isin(files_with_seizures)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = remove_bckg_files(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = remove_bckg_files(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate channels intersection and proper sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edf_info(df):\n",
    "    files = df[\"full_path\"].unique()\n",
    "\n",
    "    edf_data = []\n",
    "\n",
    "    for file in tqdm_notebook(files):\n",
    "        edf = mne.io.read_raw_edf(file, verbose=\"ERROR\")\n",
    "        data = {\n",
    "            field: edf.info[field]\n",
    "            for field in [\"ch_names\", \"sfreq\"]\n",
    "        }\n",
    "        edf_data.append(data)\n",
    "        \n",
    "    return pd.DataFrame(edf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a2dbdd62ca45b4aa8738226a1abe61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=458.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_edf_df = get_edf_info(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7300605906d145f5ba2b987951d58a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_edf_df = get_edf_info(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_FREQUENCY = int(min(train_edf_df[\"sfreq\"].min(), val_edf_df[\"sfreq\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([\n",
    "    channel for channels_list in train_edf_df[\"ch_names\"] for channel in channels_list\n",
    "] + [\n",
    "    channel for channels_list in val_edf_df[\"ch_names\"] for channel in channels_list\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = dict(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_limit = max([v for k, v in all_channels.items() if \"STI\" not in k])\n",
    "CHANNELS = [k for k, v in all_channels.items() if v >= usage_limit and \"STI\" not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STIM_CHANNEL = [k for k in all_channels.keys() if \"STI\" in k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"duration\"] = train_df[\"end\"] - train_df[\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "bckg    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "seiz    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: duration, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.groupby('label')[\"duration\"].plot(kind=\"hist\", bins=np.linspace(0, 50), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_TIME = 10 * CHUNK_FREQUENCY # number of terms per chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get all file names, randomize them\n",
    "- For each file - split on chunks, randomize them\n",
    "- Get labels for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFT_STEPS_NUM = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Fourier transform for chunk instead of full file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_transform(data, window_size=CHUNK_TIME, step_size=CHUNK_TIME // FFT_STEPS_NUM):\n",
    "    frequencies = []\n",
    "    for window in range(0, data.shape[0] - window_size, step_size):\n",
    "        chunk = data[window:window + window_size]\n",
    "        frequency_values = np.abs(np.fft.fft(chunk, axis=0))[:window_size // 2]\n",
    "        frequencies.append(frequency_values)\n",
    "    result = np.stack(frequencies)\n",
    "    return result.reshape(result.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, file, channels=CHANNELS, chunk_size=CHUNK_TIME):\n",
    "    annotations = df[\n",
    "        (df[\"full_path\"] == file) & \\\n",
    "        (df[\"label\"] == \"seiz\")\n",
    "    ][[\"start\", \"end\"]]\n",
    "    edf = mne.io.read_raw_edf(file, preload=True, verbose='ERROR')\n",
    "    edf.filter(2, 60)\n",
    "    edf_picks = edf.pick_channels(channels)\n",
    "    data, time = edf_picks[:]\n",
    "        \n",
    "    events = time * 0\n",
    "    for _, (start, end) in annotations.iterrows():\n",
    "        events += (time >= start) & (time <= end)\n",
    "    events = (events > 0).astype(int)\n",
    "    \n",
    "    return data, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_multiple(df, files, channels=CHANNELS, chunk_size=CHUNK_TIME):\n",
    "    total_data = []\n",
    "    total_events = []\n",
    "    for file in tqdm_notebook(files):\n",
    "        data, events = get_data(df, file)\n",
    "        total_data.append(data)\n",
    "        total_events.append(events)\n",
    "    \n",
    "    min_length = min([e.shape[0] for e in total_events])\n",
    "    truncated_length = (min_length // chunk_size) * chunk_size\n",
    "    total_data = [d[:, :truncated_length] for d in total_data]\n",
    "    total_events = [e[:truncated_length] for e in total_events]\n",
    "    \n",
    "    return np.stack(total_data), np.stack(total_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_chunks(data, events, chunk_size=CHUNK_TIME, step_size=CHUNK_TIME // FFT_STEPS_NUM, tqdm_enabled=False):\n",
    "    max_time = max([e.shape[0] for e in events])\n",
    "    while True:\n",
    "        iterations = range(0, max_time - 2*chunk_size, chunk_size)\n",
    "        if tqdm_enabled:\n",
    "            iterations = tqdm(iterations)\n",
    "        for chunk_start in iterations:\n",
    "            # Get two subsequent chunks, transform them into frequencies\n",
    "            data_chunk = [d[:, chunk_start:chunk_start + 2*chunk_size].T for d in data]\n",
    "            data_chunk = [get_fourier_transform(d) for d in data_chunk]\n",
    "\n",
    "            labels_chunk = [e[chunk_start:chunk_start + chunk_size] for e in events]\n",
    "            labels_chunk = [e.reshape(step_size, -1).sum(axis=0) > 0 for e in labels_chunk]\n",
    "\n",
    "            yield np.stack(data_chunk), np.stack(labels_chunk)[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence should be padded with zeros, chunks with only zeros should be excluded from loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_files(df, batch_size=BATCH_SIZE):\n",
    "    files = df[\"full_path\"].unique()\n",
    "    files = np.random.choice(files, len(files), replace=False)\n",
    "    for files in zip(*[iter(files)]*batch_size):\n",
    "        yield files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = next(iterate_files(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, events = get_data(train_df, files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040b898488ff4039b2078add4d7d8a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = train_df[\"full_path\"].value_counts().index[0:32]\n",
    "data, events = get_data_multiple(train_df, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iterate_chunks(data, events, tqdm_enabled=True)\n",
    "\n",
    "data_chunk, labels_chunk = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чанк для LSTM: (BATCH_SIZE x CHUNK_TIME x FREQUENCIES * CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_FILTERS = (8, 16, 32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIMS = (1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли добавить CNN?\n",
    "- Conv1D: (BATCH_SIZE, CHUNK_TIME, CHANNELS) -> (BATCH_SIZE, CHUNK_TIME, CNN_FILTERS)\n",
    "- MaxPool1D: (BATCH_SIZE, CHUNK_TIME, CNN_FILTERS) -> (BATCH_SIZE, CHUNK_TIME // POOL_SIZE, CNN_FILTERS)\n",
    "\n",
    "Продолжать до тех пор, пока чанка не станет достаточного размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.layers import LSTM,Bidirectional #could try TimeDistributed(Dense(...))\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers,regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend.tensorflow_backend as KTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(HIDDEN_DIMS[0], W_regularizer=regularizers.l2(l=0.01), batch_input_shape=(BATCH_SIZE, FFT_STEPS_NUM, CHUNK_TIME // 2 * len(CHANNELS))))\n",
    "model.add(Bidirectional(LSTM(RNN_SIZE, return_sequences=True, stateful=True)))#, input_shape=(seqlength, features)) ) ### bidirectional ---><---\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(HIDDEN_DIMS[1], activation='relu',W_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy', recall_m, precision_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Try to filter incomplete chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO make model converge on this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iterate_chunks(data, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generator)\n",
    "x, y_end = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_end.sum() / (y_end > -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x)[3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x, y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(x, y_end, batch_size=x.shape[0], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_end.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN использовать можно и даже немного нужно\n",
    "А вот seq2seq не надо - у вас же одинаковая длина входа и выхода. Имплементация будет очень похожа не языковую модельку, как была в последней домашке.\n",
    "\n",
    "Если это исследования, а не в прод катить, я бы попробовал LMU и LSTM-SHA из реккурентных и Sparse Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_chunks(size):\n",
    "    return (size // CHUNK_TIME - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    for train_files, val_files in tqdm_notebook(list(zip(iterate_files(train_df), iterate_files(val_df)))):\n",
    "        train_data, train_events = get_data_multiple(train_df, train_files)\n",
    "        train_generator = iterate_chunks(train_data, train_events)\n",
    "\n",
    "        model.fit_generator(\n",
    "            train_generator, \n",
    "            epochs=1, \n",
    "            steps_per_epoch=number_of_chunks(train_events[0].shape[0]),\n",
    "        )                \n",
    "        model.reset_states()\n",
    "        \n",
    "        val_data, val_events = get_data_multiple(val_df, val_files)\n",
    "        val_generator = iterate_chunks(val_data, val_events, tqdm_enabled=True)\n",
    "        \n",
    "        print(model.evaluate_generator(\n",
    "            val_generator,\n",
    "            steps=number_of_chunks(val_events[0].shape[0])\n",
    "        ))\n",
    "        \n",
    "        model.reset_states()\n",
    "        \n",
    "    model.save_weights(\"./models/lstm-fft-model-{}.h5\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Участки с нулевым precision/recall - почему val и train одинаково содержат/не содержат нули? - удалены временно\n",
    "- Высокий precision/recall на валидации не соответствует действительности - проверить!!!!\n",
    "- Влияет ли присутствие одинаковых пациентов/сессий в train/test? - проверить!!!!\n",
    "- Резко падает precision/recall - какие участки дают такой эффект?\n",
    "\n",
    "\n",
    "- Файлы во всю длину, не учитывать loss \n",
    "- Собирать чанки tN файлов в один файл\n",
    "- Переименовывать каналы других интерфейсов\n",
    "- Влияние масштабирования сигнала? \n",
    "\n",
    "\n",
    "- Можно ли использовать чанку большего размера? Можно ли ее предварительно сжать с помощью CNN? Как это повлияет на предсказание?\n",
    "\n",
    "- Применима ли к данным фильтрация? Шум сети/моргания-движения/?\n",
    "- Имеет ли смысл скалировать данные?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = next(iterate_files(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b892ba1a2a0f4c5198c658f580be3063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data, test_events = get_data_multiple(val_df, val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGenerator():\n",
    "    def __init__(self, data, events):\n",
    "        self.events = []\n",
    "        self.generator = iterate_chunks(data, events, tqdm_enabled=True)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data_chunk, events_chunk = next(self.generator)\n",
    "        self.events.append(events_chunk)\n",
    "        return data_chunk, events_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "\n",
    "test_generator = TestGenerator(train_data, train_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks(test_events.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44004a6b9b094b3d89bd5f089746551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2997f7e270408691205808e313d914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = model.predict_generator(\n",
    "    test_generator,\n",
    "    number_of_chunks(test_events.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_events = np.stack(test_generator.events[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_events = prediction.reshape(*all_true_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_true_events > 0.5).sum() / (all_true_events > -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(all_predicted_states[all_true_states > 0.5], label=\"True\")\n",
    "sns.distplot(all_predicted_states[all_true_states < 0.5], label=\"False\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare focal loss params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_focal_loss(y_true, y_pred, gamma, alpha):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    p1 = y_pred[(y_true > 0.5)]\n",
    "    p0 = y_pred[(y_true < 0.5)]\n",
    "    p1_mean = - np.sum(alpha * ((1 - p1) ** gamma) * np.log(p1))\n",
    "    p0_mean = - np.sum((1 - alpha) * (p0 ** gamma) * np.log(1 - p0))\n",
    "    return (p1_mean + p0_mean) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 1000\n",
    "y_true = (np.random.rand(shape) < 0.3).astype(float)\n",
    "y_pred_1 = 0.5 * np.ones(y_true.shape)\n",
    "y_pred_2 = 0.1 * np.ones(y_true.shape) # Should be greater than 1\n",
    "y_pred_3 = 0.49 * np.ones(y_true.shape) + (y_true > 0.5) * (0.9 - 0.49) # Should be less than 1\n",
    "y_pred_4 = 0.99 * np.ones(y_true.shape) # Should be greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17328679513998635"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_1, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3279382440610576"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_2, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11498288055263572"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_3, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5660729280736512"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_4, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare small MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iterate_chunks(data, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generator)\n",
    "x, y = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), solver=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = (np.squeeze(y).mean(axis=1) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.sum() / new_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = x.reshape(32, 250 * 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_x, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = eeg_input\n",
    "\n",
    "for cnn_filters in CNN_FILTERS:\n",
    "    feature_extractor = keras.layers.Conv1D(cnn_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(feature_extractor)\n",
    "    feature_extractor = keras.layers.Conv1D(cnn_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(feature_extractor)\n",
    "    feature_extractor = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_h_state, encoder_c_state = keras.layers.LSTM(RNN_SIZE, stateful=True, return_state=True)(feature_extractor)\n",
    "\n",
    "events_input = keras.layers.Input(batch_shape=(BATCH_SIZE, CHUNK_TIME, 1), name=\"events\")\n",
    "\n",
    "decoder_outputs, _, _ = keras.layers.LSTM(RNN_SIZE, return_sequences=True, return_state=True)(\n",
    "    events_input, \n",
    "    initial_state=[encoder_h_state, encoder_c_state]\n",
    ")\n",
    "decoder_outputs = keras.layers.Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "\n",
    "model = keras.models.Model(inputs=[eeg_input, events_input], outputs=[decoder_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.layers import Dense,Activation,Dropout\n",
    "# from keras.layers import LSTM,Bidirectional #could try TimeDistributed(Dense(...))\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras import optimizers,regularizers\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32,W_regularizer=regularizers.l2(l=0.01), batch_input_shape=(BATCH_SIZE, CHUNK_TIME // 2, len(CHANNELS))))\n",
    "# model.add(Bidirectional(LSTM(32, return_sequences=True, stateful=True)))#, input_shape=(seqlength, features)) ) ### bidirectional ---><---\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(64, activation='relu',W_regularizer=regularizers.l2(l=0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=0, alpha=0.5):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "#         total_sum = - K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \n",
    "#         total_sum -= K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "#         return 2 * total_sum / tf.cast(K.size(y_true), dtype=tf.float32)\n",
    "#     return focal_loss_fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
