{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow dataset\n",
    "- Simple LSTM Network\n",
    "- Long sequences to short chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.0MB 223kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.16.1)\n",
      "Collecting pytz>=2011k (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 168kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-0.24.2 pytz-2019.3\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mne\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/af/9c64ac8f75b1c932ca5fb16bc27740cd9b9817f9173a6608ae999e82bb6a/mne-0.20.0-py3-none-any.whl (6.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.6MB 841kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.5/dist-packages (from mne) (1.16.1)\n",
      "Collecting scipy>=0.17.1 (from mne)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/60/8cbf00c0deb50a971e6e3a015fb32513960a92867df979870a454481817c/scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 26.0MB 592kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: scipy, mne\n",
      "Successfully installed mne-0.20.0 scipy-1.4.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/ec/32310181e803f5d22e0dd33eb18924489b2f8d08cf5b6e116a93a6a5d1c6/scikit_learn-0.22.2.post1-cp35-cp35m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.0MB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.16.1)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 3.3MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-0.14.1 scikit-learn-0.22.2.post1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/86/43b8c9138ef4c2a1c492fee92792c83c13799d0e2061ff810d3826d06cd1/seaborn-0.9.1-py2.py3-none-any.whl (216kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.16.1)\n",
      "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.5/dist-packages (from seaborn) (3.0.2)\n",
      "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.5/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas>=0.17.1->seaborn) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas>=0.17.1->seaborn) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (2.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib>=1.5.3->seaborn) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas>=0.17.1->seaborn) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.3->seaborn) (40.8.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K    100% |████████████████████████████████| 378kB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.1)\n",
      "Collecting pyyaml (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 390kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.3.1 pyyaml-5.3.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 698kB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.45.0\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие каналы содержат "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "- Один сэмпл содержит чанк сигнала, чанки не пересекаются\n",
    "- Считывание происходит из случайных файлов из списка\n",
    "- Чанки рандомизированы:\n",
    "    - Учесть рандомизацию по номеру пациента, сессии, времени"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"file\", \"start\", \"end\", \"label\", \"confidence\"]\n",
    "train_df = pd.read_csv(\"../_DOCS/ref_train.txt\", sep=\" \", names=header)\n",
    "val_df = pd.read_csv(\"../_DOCS/ref_dev.txt\", sep=\" \", names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(full_file):\n",
    "    parts = full_file.split(\"_\")\n",
    "    patient = int(parts[0])\n",
    "    session = int(parts[1][1:])\n",
    "    file = int(parts[2][1:])\n",
    "    return [patient, session, file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 2, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_file(\"00000258_s002_t000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_file_info(df):\n",
    "    values = np.array(df[\"file\"].apply(preprocess_file).tolist())\n",
    "    files_df = pd.DataFrame(values, columns=[\"patient\", \"session\", \"chunk\"], index=df.index)\n",
    "    return df.merge(files_df, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = append_file_info(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = append_file_info(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO add other electrode formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_files(df, dataset):\n",
    "    paths = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(\"../edf/{}\".format(dataset)):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            if \".edf\" in file:\n",
    "                name = file.split(\".\")[0]\n",
    "                paths[name] = os.path.abspath(root) + \"/\" +  file\n",
    "    \n",
    "    df[\"full_path\"] = df[\"file\"].apply(paths.get)\n",
    "    return df[df[\"full_path\"].apply(lambda x: \"01_tcp_ar\" in str(x))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = attach_files(train_df, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = attach_files(val_df, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients = train_df[\"patient\"].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_patients = val_df[\"patient\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df[~val_df[\"patient\"].isin(train_patients)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bckg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bckg_files(df):\n",
    "    files_with_seizures = df[df[\"label\"] == \"seiz\"][\"file\"].unique()\n",
    "    return df[df[\"file\"].isin(files_with_seizures)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = remove_bckg_files(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = remove_bckg_files(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate channels intersection and proper sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edf_info(df):\n",
    "    files = df[\"full_path\"].unique()\n",
    "\n",
    "    edf_data = []\n",
    "\n",
    "    for file in tqdm_notebook(files):\n",
    "        edf = mne.io.read_raw_edf(file, verbose=\"ERROR\")\n",
    "        data = {\n",
    "            field: edf.info[field]\n",
    "            for field in [\"ch_names\", \"sfreq\"]\n",
    "        }\n",
    "        edf_data.append(data)\n",
    "        \n",
    "    return pd.DataFrame(edf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b1837bc1754e9687819be0a803776e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=458.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_edf_df = get_edf_info(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc8d28265f4a9db0182934a8f3b4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_edf_df = get_edf_info(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_FREQUENCY = int(min(train_edf_df[\"sfreq\"].min(), val_edf_df[\"sfreq\"].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([\n",
    "    channel for channels_list in train_edf_df[\"ch_names\"] for channel in channels_list\n",
    "] + [\n",
    "    channel for channels_list in val_edf_df[\"ch_names\"] for channel in channels_list\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = dict(counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_limit = max([v for k, v in all_channels.items() if \"STI\" not in k])\n",
    "CHANNELS = [k for k, v in all_channels.items() if v >= usage_limit and \"STI\" not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STIM_CHANNEL = [k for k in all_channels.keys() if \"STI\" in k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"duration\"] = train_df[\"end\"] - train_df[\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "bckg    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "seiz    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: duration, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.groupby('label')[\"duration\"].plot(kind=\"hist\", bins=np.linspace(0, 50), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_TIME = 1 * CHUNK_FREQUENCY # number of terms per chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split on chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get all file names, randomize them\n",
    "- For each file - split on chunks, randomize them\n",
    "- Get labels for each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFT_STEPS_NUM = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Fourier transform for chunk instead of full file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как западить чанку нулями, и убрать их из лосса? Варианты:\n",
    "\n",
    "### Удалять пустые чанки\n",
    "3. Заменить в массиве data и events неполные чанки на зануленные\n",
    "4. Добавить параметр mask, который отвечает за то, какие элементы функции потерь оставить\n",
    "5. Заменить лосс\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_transform(data, window_size=CHUNK_TIME, step_size=CHUNK_TIME // FFT_STEPS_NUM):\n",
    "    frequencies = []\n",
    "    for window in range(0, data.shape[0] - window_size, step_size):\n",
    "        chunk = data[window:window + window_size]\n",
    "        frequency_values = np.abs(np.fft.fft(chunk, axis=0))[:window_size // 2]\n",
    "        frequencies.append(frequency_values)\n",
    "    result = np.stack(frequencies)\n",
    "    return result.reshape(result.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, file, channels=CHANNELS, chunk_size=CHUNK_TIME):\n",
    "    annotations = df[\n",
    "        (df[\"full_path\"] == file) & \\\n",
    "        (df[\"label\"] == \"seiz\")\n",
    "    ][[\"start\", \"end\"]]\n",
    "    edf = mne.io.read_raw_edf(file, preload=True, verbose='ERROR')\n",
    "    edf.filter(2, 60)\n",
    "    edf_picks = edf.pick_channels(channels)\n",
    "    data, time = edf_picks[:]\n",
    "        \n",
    "    events = time * 0\n",
    "    for _, (start, end) in annotations.iterrows():\n",
    "        events += (time >= start) & (time <= end)\n",
    "    events = (events > 0).astype(int)\n",
    "    \n",
    "    return data, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_multiple(df, files, channels=CHANNELS, chunk_size=CHUNK_TIME):\n",
    "    total_data = []\n",
    "    total_events = []\n",
    "    for file in tqdm_notebook(files):\n",
    "        data, events = get_data(df, file)\n",
    "        total_data.append(data)\n",
    "        total_events.append(events)\n",
    "    \n",
    "    return total_data, total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunksIterator():\n",
    "    def __init__(self, data, events, chunk_size=CHUNK_TIME, step_size=CHUNK_TIME // FFT_STEPS_NUM, max_length=MAX_LENGTH, tqdm_enabled=False):\n",
    "        self.data = data\n",
    "        self.events = events\n",
    "        self.chunk_size = chunk_size\n",
    "        self.step_size = step_size\n",
    "        self.valid_chunks = 0\n",
    "        \n",
    "        max_time = min(max([e.shape[0] for e in events]), max_length)\n",
    "        self.iterations = range(0, max_time - 2*chunk_size, chunk_size)\n",
    "        self.iterations_number = len(self.iterations)\n",
    "        if tqdm_enabled:\n",
    "            self.iterations = tqdm(self.iterations)\n",
    "        self.iterations = iter(self.iterations)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data = self.data\n",
    "        events = self.events\n",
    "        chunk_size = self.chunk_size\n",
    "        step_size = self.step_size\n",
    "        chunk_start = next(self.iterations)\n",
    "        \n",
    "        data_chunk = [d[:, chunk_start:chunk_start + 2*chunk_size].T for d in data]\n",
    "        labels_chunk = [e[chunk_start:chunk_start + chunk_size] for e in events]\n",
    "\n",
    "        zero_data_chunk = np.zeros((chunk_size * 2, len(CHANNELS)))\n",
    "        zero_labels_chunk = np.zeros((chunk_size, ))\n",
    "\n",
    "        masks_chunk = [e.shape[0] == chunk_size for e in labels_chunk]\n",
    "        data_chunk = [d if d.shape[0] == 2*chunk_size else zero_data_chunk for d in data_chunk]\n",
    "        labels_chunk = [e if e.shape[0] == chunk_size else zero_labels_chunk for e in labels_chunk]\n",
    "\n",
    "        data_chunk = [get_fourier_transform(d) for d in data_chunk]\n",
    "        labels_chunk = [e.reshape(step_size, -1).sum(axis=0) > 0 for e in labels_chunk]\n",
    "        \n",
    "        self.valid_chunks += sum(masks_chunk)\n",
    "\n",
    "        return [np.stack(data_chunk), np.array(masks_chunk)], np.stack(labels_chunk)[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iterate_chunks(data, events, chunk_size=CHUNK_TIME, step_size=CHUNK_TIME // FFT_STEPS_NUM, tqdm_enabled=False):\n",
    "#     max_time = max([e.shape[0] for e in events])\n",
    "#     while True:\n",
    "#         iterations = range(0, max_time - 2*chunk_size, chunk_size)\n",
    "#         if tqdm_enabled:\n",
    "#             iterations = tqdm(iterations)\n",
    "#         for chunk_start in iterations:\n",
    "#             data_chunk = [d[:, chunk_start:chunk_start + 2*chunk_size].T for d in data]\n",
    "#             labels_chunk = [e[chunk_start:chunk_start + chunk_size] for e in events]\n",
    "            \n",
    "#             zero_data_chunk = np.zeros((chunk_size * 2, len(CHANNELS)))\n",
    "#             zero_labels_chunk = np.zeros((chunk_size, ))\n",
    "            \n",
    "#             masks_chunk = [e.shape[0] == chunk_size for e in labels_chunk]\n",
    "#             data_chunk = [d if d.shape[0] == 2*chunk_size else zero_data_chunk for d in data_chunk]\n",
    "#             labels_chunk = [e if e.shape[0] == chunk_size else zero_labels_chunk for e in labels_chunk]\n",
    "            \n",
    "#             data_chunk = [get_fourier_transform(d) for d in data_chunk]\n",
    "#             labels_chunk = [e.reshape(step_size, -1).sum(axis=0) > 0 for e in labels_chunk]\n",
    "\n",
    "#             yield [np.stack(data_chunk), np.array(masks_chunk)], np.stack(labels_chunk)[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_files(df, batch_size=BATCH_SIZE):\n",
    "    files = df[\"full_path\"].unique()\n",
    "    files = np.random.choice(files, len(files), replace=False)\n",
    "    for files in zip(*[iter(files)]*batch_size):\n",
    "        yield files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = next(iterate_files(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, events = get_data(train_df, files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d3f8884be644a5992515736f081f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = train_df[\"full_path\"].value_counts().index[0:BATCH_SIZE]\n",
    "data, events = get_data_multiple(train_df, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_lengths = [int(d.shape[1] * 0.005) for d in data]\n",
    "partial_data = [d[:, :l] for d, l in zip(data, partial_lengths)]\n",
    "partial_events = [e[:l] for e, l in zip(events, partial_lengths)]\n",
    "min(partial_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540bbff068694fdfa613b8d8b8583e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 48\n",
      "Iteration 0 32\n",
      "Iteration 1 28\n",
      "Iteration 2 27\n",
      "Iteration 3 18\n",
      "Iteration 4 16\n",
      "Iteration 5 16\n",
      "Iteration 6 10\n",
      "Iteration 7 8\n",
      "Iteration 8 6\n",
      "Iteration 9 5\n",
      "Valid chunks: 166\n"
     ]
    }
   ],
   "source": [
    "iterator = ChunksIterator(partial_data, partial_events, tqdm_enabled=True)\n",
    "\n",
    "print(\"Total iterations:\", iterator.iterations_number)\n",
    "\n",
    "for i, ((data_chunk, masks_chunk), labels_chunk) in zip(range(10), iterator):\n",
    "    print(\"Iteration {}\".format(i), sum(masks_chunk))\n",
    "    \n",
    "print(\"Valid chunks:\", iterator.valid_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чанк для LSTM: (BATCH_SIZE x CHUNK_TIME x FREQUENCIES * CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_FILTERS = (8, 16, 32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIMS = (1024, 1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли добавить CNN?\n",
    "- Conv1D: (BATCH_SIZE, CHUNK_TIME, CHANNELS) -> (BATCH_SIZE, CHUNK_TIME, CNN_FILTERS)\n",
    "- MaxPool1D: (BATCH_SIZE, CHUNK_TIME, CNN_FILTERS) -> (BATCH_SIZE, CHUNK_TIME // POOL_SIZE, CNN_FILTERS)\n",
    "\n",
    "Продолжать до тех пор, пока чанка не станет достаточного размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense,Activation,Dropout, Input\n",
    "from keras.layers import LSTM,Bidirectional #could try TimeDistributed(Dense(...))\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers,regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend.tensorflow_backend as KTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(HIDDEN_DIMS[0], W_regularizer=regularizers.l2(l=0.01), batch_input_shape=(BATCH_SIZE, FFT_STEPS_NUM, CHUNK_TIME // 2 * len(CHANNELS))))\n",
    "# model.add(Bidirectional(LSTM(RNN_SIZE, return_sequences=True, stateful=True)))#, input_shape=(seqlength, features)) ) ### bidirectional ---><---\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(HIDDEN_DIMS[1], activation='relu',W_regularizer=regularizers.l2(l=0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, kernel_regularizer=<keras.reg..., activation=\"relu\")`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "eeg_input = Input(batch_shape=(BATCH_SIZE, FFT_STEPS_NUM, CHUNK_TIME // 2 * len(CHANNELS)))\n",
    "\n",
    "masks_input = Input(batch_shape=(BATCH_SIZE, 1))\n",
    "squeezed_masks_input = tf.squeeze(masks_input)\n",
    "\n",
    "x = Dense(\n",
    "    HIDDEN_DIMS[0], \n",
    "    W_regularizer=regularizers.l2(l=0.01),\n",
    ")(eeg_input)\n",
    "x = Bidirectional(LSTM(RNN_SIZE, return_sequences=True, stateful=True))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(HIDDEN_DIMS[1], activation='relu',W_regularizer=regularizers.l2(l=0.01))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "events_output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[eeg_input, masks_input], outputs=[events_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = data_chunk\n",
    "# b = data_chunk\n",
    "# c = [0] * 10 + [1] * 22\n",
    "# binary_crossentropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=a, logits=b)\n",
    "# masked_binary_crossentropy_loss = tf.boolean_mask(binary_crossentropy_loss, c, axis=0)\n",
    "# s.run(masked_binary_crossentropy_loss).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_binary_crossentropy(y_true, y_pred):\n",
    "    global squeezed_masks_input\n",
    "    binary_crossentropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    masked_binary_crossentropy_loss = tf.boolean_mask(binary_crossentropy_loss, squeezed_masks_input, axis=0)\n",
    "    return tf.reduce_mean(masked_binary_crossentropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=masked_binary_crossentropy,\n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy', recall_m, precision_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Try to filter incomplete chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit on small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO make model converge on this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ChunksIterator(data, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    next(generator)\n",
    "(x,  masks), y_end = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_end.sum() / (y_end > -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998581"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([x, masks]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/32 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.575618743896484, 0.53125, 0.0, 0.0]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([x, masks], y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(x, y_end, batch_size=x.shape[0], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_end.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN использовать можно и даже немного нужно\n",
    "А вот seq2seq не надо - у вас же одинаковая длина входа и выхода. Имплементация будет очень похожа не языковую модельку, как была в последней домашке.\n",
    "\n",
    "Если это исследования, а не в прод катить, я бы попробовал LMU и LSTM-SHA из реккурентных и Sparse Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870d7c18c1e0445bacbb25335e0b228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32992a2539444bfab1dfbd5e3dc1a72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5199fa105442c1a588644d4941fce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5564a94d424997b139d695f19fbd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=398.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "201/398 [==============>...............] - ETA: 2:41 - loss: 1.9613 - accuracy: 0.6898 - recall_m: 0.4380 - precision_m: 0.5693"
     ]
    }
   ],
   "source": [
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    total_train_loss = 0\n",
    "    total_train_chunks = 0\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    total_val_chunks = 0\n",
    "    \n",
    "    for train_files, val_files in tqdm_notebook(list(zip(iterate_files(train_df), iterate_files(val_df)))):\n",
    "        # Training\n",
    "        \n",
    "        train_data, train_events = get_data_multiple(train_df, train_files)\n",
    "        train_generator = ChunksIterator(train_data, train_events, tqdm_enabled=True)\n",
    "        \n",
    "        train_chunks = train_generator.iterations_number\n",
    "        \n",
    "        train_history = model.fit_generator(\n",
    "            train_generator, \n",
    "            epochs=1, \n",
    "            steps_per_epoch=train_chunks,\n",
    "        )\n",
    "        \n",
    "        total_train_loss += train_history.history['loss'][0] * train_generator.valid_chunks\n",
    "        total_train_chunks += train_generator.valid_chunks\n",
    "        \n",
    "        model.reset_states()\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        val_data, val_events = get_data_multiple(val_df, val_files)\n",
    "        val_generator = ChunksIterator(val_data, val_events, tqdm_enabled=True)\n",
    "        \n",
    "        val_chunks = val_generator.iterations_number\n",
    "        \n",
    "        val_metrics = model.evaluate_generator(\n",
    "            val_generator,\n",
    "            steps=val_chunks\n",
    "        )\n",
    "        \n",
    "        total_val_loss += val_metrics[0] * val_generator.valid_chunks\n",
    "        total_val_chunks += val_generator.valid_chunks\n",
    "        \n",
    "        model.reset_states()\n",
    "        \n",
    "        print(\"Train loss:\", total_train_loss / total_train_chunks)\n",
    "        print(\"Val loss:\", total_val_loss / total_val_chunks)\n",
    "    \n",
    "    train_history.append(total_train_loss / total_train_chunks)\n",
    "    val_history.append(total_val_loss / total_val_chunks)\n",
    "    model.save_weights(\"./models/lstm-fft-model-{}.h5\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверено\n",
    "- Участки с нулевым precision/recall - почему val и train одинаково содержат/не содержат нули? - удалены временно\n",
    "- Влияет ли присутствие одинаковых пациентов/сессий в train/test? - разбиты на группы\n",
    "- Файлы во всю длину, не учитывать loss - готово\n",
    "- Собирать чанки tN файлов в один файл - не нужно пока что\n",
    "\n",
    "# Проверить\n",
    "- Модель переобучается, как можно ее уменьшить?\n",
    "- Переименовывать каналы других интерфейсов - проверить различные интерфейсы на предмет соответствия каналов\n",
    "- Влияние масштабирования сигнала \n",
    "- Можно ли использовать чанку большего размера? Можно ли ее предварительно сжать с помощью CNN? Как это повлияет на предсказание?\n",
    "- Применима ли к данным фильтрация? моргания-движения/?\n",
    "- Расстояние между приступами - не менее 3с, можно ли это использовать для сглаживания сегментов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_files = next(iterate_files(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52c8f4d520744c0a0e930eb6fdb3186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data, test_events = get_data_multiple(train_df, train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGenerator():\n",
    "    def __init__(self, data, events):\n",
    "        self.events = []\n",
    "        self.generator = iterate_chunks(data, events, tqdm_enabled=True)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        data_chunk, events_chunk = next(self.generator)\n",
    "        self.events.append(events_chunk)\n",
    "        return data_chunk, events_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "\n",
    "test_generator = TestGenerator(train_data, train_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_chunks(test_events.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_events.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a56cb8fa4be44e0a878b4c491fa7c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28c3a25d79c44229f253e3ada76eddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = model.predict_generator(\n",
    "    test_generator,\n",
    "    number_of_chunks(test_events.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_events = np.stack(test_generator.events[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_events = prediction.reshape(*all_true_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_true_events > 0.5).sum() / (all_true_events > -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predicted_events[all_true_events > 0.5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_events[all_true_events < 0.5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f423c4002b0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(all_predicted_events[all_true_events > 0.5], label=\"True\")\n",
    "sns.distplot(all_predicted_events[all_true_events < 0.5], label=\"False\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_events_line = all_true_events[:, 0, :, :].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_events_line = all_predicted_events[:, 0, :, :].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f423c220c18>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAApCAYAAAAViMPvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABVdJREFUeJzt3V2IVGUcx/Hvb8eXzY1SS8RWSyOxJCpD1CgitMhKqouoxECi6KIgiyKsi6CLgiB6uYjAtPAiLDMh8yIIU+giNjUv8qVS7MUVTcOsMPKtfxfnbLMtu+7Zcc7Mnj2/z83umXnmnP8++58/Z57znGcUEZiZWXG0NDsAMzMbGBduM7OCceE2MysYF24zs4Jx4TYzKxgXbjOzgnHhNjMrmEyFW9J8Sd9J2iNpad5BmZlZ39TfDTiSKsD3wC1AJ7AZWBgRO/MPz8zMehqWoc0sYE9E7AWQ9D5wF9Bn4R6hkdFKW30iNDMrgb85xok4rixtsxTudmBft+1OYPaZXtBKG7M1L8vxzcwM6IgNmdtmGeN+FFgkafuZGkl6RNIWSVtOcjxzAGZmNjBZzrg/BFqB89LticD+no0iYhmwDOCKq0bGS598BcBpMp3510WF2hbMyvq6Fg18/7XE1FLr39Gg+CD7dKRKxn9/5v1lPm7vB651GlWlhjxu6SOG3I+b82Sxvvq2mU5nXCwva+xZ+7Al4/+nov73N+vWvzLtKzlu/94mKdbDJY0A7gfWZT6CmZnVVb+FOyJOAc8Dk4FdwOqI2NGzXfehkqNHTtc9UDMzS2QZKgHYBOyOiCv7atBzqOTsQzMzs970O48bQNJkYP2ZCneP9oeBY8CvZxPcEHIh7osu7osq90WV+wIuiYhxWRpmPeMekIgYJ2lLRMzMY/9F476ocl9UuS+q3BcD0+8Yt6RVwJfANEmdkh7KPywzM+tLv2fcEbGwEYGYmVk2eU74XJbjvovGfVHlvqhyX1S5LwYg08VJMzMbPLwet5lZweRSuMu8frekSZI2StopaYekJenjYyV9Jml3+nNMs2NtBEkVSdskrU+3p0jqSHPjg/Ru3FKQNFrSGknfStol6boy5oWkJ9P3xnZJqyS1ljkvalH3wp2u3/0mcBswHVgoaXq9jzOInQKeiojpwBzgsfTvXwpsiIipwIZ0uwyWkNxx2+Vl4LWIuAz4DSjTLKU3gE8j4nLgapJ+KVVeSGoHHgdmpveFVEiW0ShzXgxYHmfc/63fHREngK71u0shIg5ExNfp73+SvDnbSfpgZdpsJXB3cyJsHEkTgTuA5em2gLnAmrRJKfoBQNL5wI3ACoCIOBERRylhXpDMZjtH0jBgFHCAkuZFrfIo3L2t392ew3EGvfSO0xlABzA+Ig6kTx0ExjcprEZ6HXgG+CfdvgA4mq5/A+XKjSnAYeDddOhouaQ2SpYXEbEfeAX4maRg/w5spbx5URNfnMyJpHOBj4AnIuKP7s9FMpVnSE/nkbQAOBQRW5sdyyAxDLgWeCsiZpAsCfG/YZGS5MUYkk8ZU4CLgDZgflODKqA8Cvd+YFK37V7X7x7KJA0nKdrvRcTa9OFfJE1In58AHGpWfA1yPXCnpB9Jhsvmkozxjk4/IkO5cqMT6IyIjnR7DUkhL1te3Az8EBGHI+IksJYkV8qaFzXJo3BvBqamV4lLt353Oo67AtgVEa92e2odsDj9fTHwcaNja6SIeDYiJkbEZJIc+DwiFgEbgXvSZkO+H7pExEFgn6Rp6UPzSL63tVR5QTJEMkfSqPS90tUPpcyLWuVyA46k20nGNyvAOxHxYt0PMkhJugH4AviG6tjucyTj3KuBi4GfgHsj4khTgmwwSTcBT0fEAkmXkpyBjwW2AQ9ERCm+607SNSQXakcAe4EHSU6eSpUXkl4A7iOZgbUNeJhkTLuUeVEL3zlpZlYwvjhpZlYwLtxmZgXjwm1mVjAu3GZmBePCbWZWMC7cZmYF48JtZlYwLtxmZgXzL57jq2KG2zKXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(true_events_line)\n",
    "plt.plot(predicted_events_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "In time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare focal loss params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_focal_loss(y_true, y_pred, gamma, alpha):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    p1 = y_pred[(y_true > 0.5)]\n",
    "    p0 = y_pred[(y_true < 0.5)]\n",
    "    p1_mean = - np.sum(alpha * ((1 - p1) ** gamma) * np.log(p1))\n",
    "    p0_mean = - np.sum((1 - alpha) * (p0 ** gamma) * np.log(1 - p0))\n",
    "    return (p1_mean + p0_mean) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 1000\n",
    "y_true = (np.random.rand(shape) < 0.3).astype(float)\n",
    "y_pred_1 = 0.5 * np.ones(y_true.shape)\n",
    "y_pred_2 = 0.1 * np.ones(y_true.shape) # Should be greater than 1\n",
    "y_pred_3 = 0.49 * np.ones(y_true.shape) + (y_true > 0.5) * (0.9 - 0.49) # Should be less than 1\n",
    "y_pred_4 = 0.99 * np.ones(y_true.shape) # Should be greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17328679513998635"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_1, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3279382440610576"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_2, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11498288055263572"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_3, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5660729280736512"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_focal_loss(y_true, y_pred_4, gamma=gamma, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare small MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = iterate_chunks(data, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generator)\n",
    "x, y = next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), solver=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = (np.squeeze(y).mean(axis=1) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y.sum() / new_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = x.reshape(32, 250 * 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_x, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(new_x, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = eeg_input\n",
    "\n",
    "for cnn_filters in CNN_FILTERS:\n",
    "    feature_extractor = keras.layers.Conv1D(cnn_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(feature_extractor)\n",
    "    feature_extractor = keras.layers.Conv1D(cnn_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(feature_extractor)\n",
    "    feature_extractor = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_h_state, encoder_c_state = keras.layers.LSTM(RNN_SIZE, stateful=True, return_state=True)(feature_extractor)\n",
    "\n",
    "events_input = keras.layers.Input(batch_shape=(BATCH_SIZE, CHUNK_TIME, 1), name=\"events\")\n",
    "\n",
    "decoder_outputs, _, _ = keras.layers.LSTM(RNN_SIZE, return_sequences=True, return_state=True)(\n",
    "    events_input, \n",
    "    initial_state=[encoder_h_state, encoder_c_state]\n",
    ")\n",
    "decoder_outputs = keras.layers.Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "\n",
    "model = keras.models.Model(inputs=[eeg_input, events_input], outputs=[decoder_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.layers import Dense,Activation,Dropout\n",
    "# from keras.layers import LSTM,Bidirectional #could try TimeDistributed(Dense(...))\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras import optimizers,regularizers\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32,W_regularizer=regularizers.l2(l=0.01), batch_input_shape=(BATCH_SIZE, CHUNK_TIME // 2, len(CHANNELS))))\n",
    "# model.add(Bidirectional(LSTM(32, return_sequences=True, stateful=True)))#, input_shape=(seqlength, features)) ) ### bidirectional ---><---\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(64, activation='relu',W_regularizer=regularizers.l2(l=0.01)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=0, alpha=0.5):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "#         total_sum = - K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \n",
    "#         total_sum -= K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "#         return 2 * total_sum / tf.cast(K.size(y_true), dtype=tf.float32)\n",
    "#     return focal_loss_fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
